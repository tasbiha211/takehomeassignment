{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate JAVA runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: /Users/tasbihaasim/.sdkman/candidates/java/current\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"17\" 2021-09-14\n",
      "OpenJDK Runtime Environment Temurin-17+35 (build 17+35)\n",
      "OpenJDK 64-Bit Server VM Temurin-17+35 (build 17+35, mixed mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set Java environment variables\n",
    "java_home = '/Users/tasbihaasim/.sdkman/candidates/java/current'\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "os.environ['PATH'] = f\"{java_home}/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Check if JAVA_HOME is set correctly\n",
    "print(\"JAVA_HOME:\", os.environ.get('JAVA_HOME'))\n",
    "\n",
    "# Check if the correct java binary is in PATH\n",
    "os.system('java -version')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,BooleanType,DoubleType\n",
    "from pyspark.sql.functions import col, weekofyear, year\n",
    "from pyspark.sql.functions import to_timestamp, date_format, concat_ws, col, to_date\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse(session):\n",
    "    dataframe = session.read.json(\"dataset.json\")\n",
    "    # Print the original schema\n",
    "    dataframe.printSchema()\n",
    "    \n",
    "    # Convert the 'timestamp' column to timestamp type and add it as a new column\n",
    "\n",
    "    \n",
    "    dataframe = dataframe.withColumn(\"new_timestamp\", to_date(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\n",
    "    \n",
    "    # Print the schema after the conversion\n",
    "    dataframe.printSchema()\n",
    "    # cast the timstampt column to be of a type timestamp\n",
    "    # dataframe.show()\n",
    "    app_loaded_events = dataframe.filter(dataframe.event == \"app_loaded\")\n",
    "    registered_events = dataframe.filter(dataframe.event == \"registered\")\n",
    "    app_loaded_events.write.mode(\"overwrite\").parquet(\"user/events/app_loaded/\")\n",
    "    registered_events.write.mode(\"overwrite\").parquet(\"user/events/registered/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(session):\n",
    "    # Load the parquet files into DataFrames\n",
    "    app_loaded_df = session.read.parquet(\"user/events/app_loaded/\")\n",
    "    registered_df = session.read.parquet(\"user/events/registered/\")\n",
    "\n",
    "    # Extract year and week from the timestamp\n",
    "    app_loaded_df = app_loaded_df.withColumn(\"year\", year(\"new_timestamp\")).withColumn(\"week_of_year\", weekofyear(\"new_timestamp\"))\n",
    "    registered_df = registered_df.withColumn(\"year\", year(\"new_timestamp\")).withColumn(\"week_of_year\", weekofyear(\"new_timestamp\"))\n",
    "\n",
    "    # Create temporary views for SQL operations\n",
    "    app_loaded_df.createOrReplaceTempView(\"apploaded\")\n",
    "    registered_df.createOrReplaceTempView(\"registered\")\n",
    "    \n",
    "    # # Run the first query to get the last app load time for each user\n",
    "    # lastload = '''\n",
    "    # SELECT initiator_id, MAX(new_timestamp) AS last_app_load_time\n",
    "    # FROM apploaded\n",
    "    # GROUP BY initiator_id\n",
    "    # '''\n",
    "\n",
    "    # lastload_df = session.sql(lastload)\n",
    "    # lastload_df.createOrReplaceTempView(\"lastload_df\")  # Using 'lastload_df' as the view name\n",
    "\n",
    "    # total_non_distinct_appload_events = '''\n",
    "    # SELECT \n",
    "    # initiator_id, \n",
    "    # COUNT(*) AS load_count\n",
    "    # FROM \n",
    "    #     apploaded\n",
    "    # GROUP BY \n",
    "    #     initiator_id\n",
    "    # HAVING \n",
    "    #     COUNT(*) > 1;\n",
    "    # '''\n",
    "    # print(\"users who loaded the app more than once\")\n",
    "    # countt = session.sql(total_non_distinct_appload_events)\n",
    "    # countt.createOrReplaceTempView(\"countt\")\n",
    "    # countt.show()\n",
    "\n",
    "    # # Debugging: Check the content of the 'lastload_df' view\n",
    "    # print(\"Last Load DF:\")\n",
    "    # lastload_df.show()\n",
    "\n",
    "    # query = '''\n",
    "    # SELECT r.initiator_id\n",
    "    # FROM registered r\n",
    "    # JOIN apploaded l\n",
    "    # ON r.initiator_id = l.initiator_id\n",
    "    # WHERE \n",
    "    #     (r.year = year(l.last_app_load_time) AND r.week_of_year = weekofyear(l.last_app_load_time) - 1)\n",
    "    #     OR \n",
    "    #     ((r.year = year(l.last_app_load_time) + 1) AND (weekofyear(l.last_app_load_time) = 52 AND r.week_of_year = 1))\n",
    "    # '''\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT r.initiator_id\n",
    "    FROM registered r\n",
    "    JOIN apploaded a\n",
    "    ON r.initiator_id = a.initiator_id\n",
    "    AND (r.year = a.year AND r.week_of_year = a.week_of_year-1)\n",
    "\n",
    "    \"\"\"\n",
    "    dfsql = session.sql(query)  # This should refer to 'lastload_df'\n",
    "\n",
    "    # Debugging: Check the result of the query\n",
    "    print(\"Filtered DF:\")\n",
    "    dfsql.show()\n",
    "\n",
    "    # Run the query to get all users\n",
    "    query2 = '''\n",
    "    SELECT DISTINCT initiator_id FROM registered\n",
    "    '''\n",
    "    dfsql_all_users = session.sql(query2)\n",
    "\n",
    "    # Calculate the statistics\n",
    "    registered_users = dfsql.count()  # users that registered themselves the week after loading the app\n",
    "    total_users_count = dfsql_all_users.count()  # total users\n",
    "\n",
    "    percentage = (registered_users / total_users_count) * 100\n",
    "\n",
    "    return percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/12 15:37:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first pyspark application\n"
     ]
    }
   ],
   "source": [
    "\n",
    "session = SparkSession.builder.appName(\"first pyspark application\")\\\n",
    ".master(\"local[*]\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "print(session.sparkContext.appName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- browser_version: string (nullable = true)\n",
      " |-- campaign: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- initiator_id: long (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- browser_version: string (nullable = true)\n",
      " |-- campaign: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- initiator_id: long (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- new_timestamp: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/12 15:38:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/08/12 15:38:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/08/12 15:38:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/08/12 15:38:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "24/08/12 15:38:10 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/08/12 15:38:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/08/12 15:38:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/08/12 15:38:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "24/08/12 15:38:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/08/12 15:38:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/08/12 15:38:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/08/12 15:38:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "24/08/12 15:38:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "24/08/12 15:38:11 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "parse(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DF:\n",
      "+-------------------+\n",
      "|       initiator_id|\n",
      "+-------------------+\n",
      "|3074457347138120171|\n",
      "|3074457347155748055|\n",
      "|3074457347157300243|\n",
      "|3074457347124748055|\n",
      "|3074457347143760172|\n",
      "|3074457347146488765|\n",
      "|3074457347153067794|\n",
      "|3074457347173997067|\n",
      "|3074457347139582185|\n",
      "|3074457347131568759|\n",
      "|3074457347132133806|\n",
      "|3074457347129021537|\n",
      "|3074457347138829175|\n",
      "|3074457347138816721|\n",
      "|3074457347172960046|\n",
      "|3074457347150067564|\n",
      "|3074457347157776707|\n",
      "|3074457347131400948|\n",
      "|3074457347138182666|\n",
      "|3074457347146917642|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "7.571409047423808\n"
     ]
    }
   ],
   "source": [
    "print(statistics(session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-------+-----------+----------+-------------------+--------------------+-------------+\n",
      "|browser_version|campaign|channel|device_type|     event|       initiator_id|           timestamp|new_timestamp|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+--------------------+-------------+\n",
      "|               |    NULL|   NULL| mobile-app|app_loaded|3074457346125985645|2020-01-07T18:18:...|   2020-07-01|\n",
      "|           65.0|    NULL|   NULL|    desktop|app_loaded|3074457347026799765|2020-01-07T18:18:...|   2020-07-01|\n",
      "|           56.0|    NULL|   NULL|desktop-app|app_loaded|3074457347134362674|2020-01-07T18:19:...|   2020-07-01|\n",
      "|           79.0|    NULL|   NULL|     mobile|app_loaded|3074457347134440587|2020-01-07T18:19:...|   2020-07-01|\n",
      "|           79.0|    NULL|   NULL|    desktop|app_loaded|3074457346043002471|2020-01-07T18:19:...|   2020-07-01|\n",
      "|           79.0|    NULL|   NULL|    desktop|app_loaded|3074457347034158585|2020-01-07T18:20:...|   2020-07-01|\n",
      "|           56.0|    NULL|   NULL|desktop-app|app_loaded|3074457346187056943|2020-01-07T18:20:...|   2020-07-01|\n",
      "|           79.0|    NULL|   NULL|    desktop|app_loaded|3074457346083359362|2020-01-07T18:21:...|   2020-07-01|\n",
      "|           79.0|    NULL|   NULL|    desktop|app_loaded|3074457346328859606|2020-01-07T18:21:...|   2020-07-01|\n",
      "|           79.0|    NULL|   NULL|    desktop|app_loaded|3074457346307943924|2020-01-07T18:22:...|   2020-07-01|\n",
      "|           56.0|    NULL|   NULL|desktop-app|app_loaded|3074457346611592915|2020-01-07T18:22:...|   2020-07-01|\n",
      "|           56.0|    NULL|   NULL|desktop-app|app_loaded|3074457346979700240|2020-01-07T18:23:...|   2020-07-01|\n",
      "|           56.0|    NULL|   NULL|desktop-app|app_loaded|3074457346315815916|2020-01-07T18:23:...|   2020-07-01|\n",
      "|           56.0|    NULL|   NULL|desktop-app|app_loaded|3074457346453213575|2020-01-07T18:25:...|   2020-07-01|\n",
      "|           78.0|    NULL|   NULL|    desktop|app_loaded|3074457347134440542|2020-01-07T18:25:...|   2020-07-01|\n",
      "|           56.0|    NULL|   NULL|desktop-app|app_loaded|3074457345862352833|2020-01-07T17:00:...|   2020-07-01|\n",
      "|           79.0|    NULL|   NULL|    desktop|app_loaded|3074457346634766459|2020-01-07T17:00:...|   2020-07-01|\n",
      "|           79.0|    NULL|   NULL|    desktop|app_loaded|3074457346267486344|2020-01-07T17:00:...|   2020-07-01|\n",
      "|           13.0|    NULL|   NULL|     mobile|app_loaded|3074457347084836227|2020-01-07T17:00:...|   2020-07-01|\n",
      "|           56.0|    NULL|   NULL|desktop-app|app_loaded|3074457346242992791|2020-01-07T17:01:...|   2020-07-01|\n",
      "+---------------+--------+-------+-----------+----------+-------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# app_loaded_df = session.read.parquet(\"user/events/app_loaded/\")\n",
    "# registered_df = session.read.parquet(\"user/events/registered/\")\n",
    "session.read.parquet(\"user/events/app_loaded/\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
